{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Features\n",
    "\n",
    "The goal of this notebook is to develop some additional features for the deep learning model, as well as generate the input files.\n",
    "\n",
    "(note: each feature includes a unit test or two at the bottom, which can be modified to better understand the feature usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some text cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loving ! \n",
      "love\n"
     ]
    }
   ],
   "source": [
    "LEMMATIZER = WordNetLemmatizer()\n",
    "LEMMATIZED_WORDS = {}\n",
    "\n",
    "LEMMATIZER_CACHE = {}\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "def quick_lemmatize(word, tag):\n",
    "    global LEMMATIZER_CACHE\n",
    "    wntag = penn_to_wn(tag)\n",
    "    if wntag is None:\n",
    "        return word\n",
    "    word_tag_tuple = (word, wntag)\n",
    "    if word_tag_tuple not in LEMMATIZER_CACHE:\n",
    "        LEMMATIZER_CACHE[word_tag_tuple] = LEMMATIZER.lemmatize(word_tag_tuple[0], word_tag_tuple[1])\n",
    "    return LEMMATIZER_CACHE[word_tag_tuple]\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return 'a'\n",
    "    elif is_noun(tag):\n",
    "        return 'n'\n",
    "    elif is_adverb(tag):\n",
    "        return 'r'\n",
    "    elif is_verb(tag):\n",
    "        return 'v'\n",
    "    return None\n",
    "\n",
    "def clean_text(text):\n",
    "    if (type(text) != type('aa')):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = ' '.join([w for w in text.split(' ') if w not in STOP_WORDS])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def do_lemmatization(text):\n",
    "    text = list(filter(lambda r: len(r) > 0 and r not in STOP_WORDS, text.split(' '))) #pos_tag requires list...\n",
    "    pos_tagged = nltk.pos_tag(text)\n",
    "    lemmatized = ' '.join(map(lambda w: quick_lemmatize(w[0], w[1]), pos_tagged))\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "print(clean_text('I am loving it!'))\n",
    "print(do_lemmatization('loving'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question type feature. This feature looks for the words 'who', 'what', 'where', 'when', 'why', and 'how' in a question. The output is a bit vector with one dimension for each word; the field is 1 if the word is in the question, 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_how_question_1      0.0\n",
      "is_what_question_1     0.0\n",
      "is_when_question_1     0.0\n",
      "is_where_question_1    0.0\n",
      "is_who_question_1      0.0\n",
      "is_why_question_1      1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "QUESTION_TYPES = ['who', 'what', 'where', 'when', 'why', 'how']\n",
    "\n",
    "def get_question_type(text, postfix):\n",
    "    if type(text) != type('aa') or len(text) == 0:\n",
    "        text = ''\n",
    "    text = re.sub(r\"[^A-Za-z0-9^]\", \" \", text.lower())\n",
    "    text = text.split(' ')\n",
    "    data = {}\n",
    "    column_format = 'is_{0}_question_{1}'\n",
    "    for i in range(0, len(QUESTION_TYPES), 1):\n",
    "        if len(text) > 0 and QUESTION_TYPES[i] in text:\n",
    "            data[column_format.format(QUESTION_TYPES[i], postfix)] = 1.0\n",
    "        else:\n",
    "            data[column_format.format(QUESTION_TYPES[i], postfix)] = 0.0\n",
    "        \n",
    "    return pd.Series(data)\n",
    "\n",
    "print(get_question_type('why do I use HDInsight?', '1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token IoU. Count the number of tokens that are the same in the words, and divide by the total distinct tokens in the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def intersection_over_union(text1, text2):\n",
    "    text1_tokens = set(text1.split(' '))\n",
    "    text2_tokens = set(text2.split(' '))\n",
    "    \n",
    "    intersect_count = len(text1_tokens.intersection(text2_tokens))\n",
    "    union_count = len(text1_tokens.union(text2_tokens))\n",
    "    \n",
    "    return intersect_count / union_count\n",
    "\n",
    "intersection_over_union('a b b c', 'b d c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longest common substring. Find the longest common token sequence in the text, divide by the minimum number of tokens in the two input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def longest_common_substring(text1, text2):\n",
    "    text1_tokens = text1.split(' ')\n",
    "    text2_tokens = text2.split(' ')\n",
    "    mat = [[0 for i in range(0, len(text2_tokens), 1)] for j in range(0, len(text1_tokens))]\n",
    "    max_substring_length = 0\n",
    "    for i in range(0, len(text1_tokens), 1):\n",
    "        for j in range(0, len(text2_tokens), 1):\n",
    "            if text1_tokens[i] == text2_tokens[j]:\n",
    "                if i > 0 and j > 0:\n",
    "                    mat[i][j] = mat[i-1][j-1] + 1.0\n",
    "                else:\n",
    "                    mat[i][j] = 1.0\n",
    "                if (mat[i][j] > max_substring_length):\n",
    "                    max_substring_length = mat[i][j]\n",
    "                    \n",
    "    return max_substring_length / float(min(len(text1_tokens), len(text2_tokens)))\n",
    "\n",
    "longest_common_substring('foo bar baz qux', 'bar baz cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question length. Number of tokens in the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_question_length(text):\n",
    "    return len(text.split(' '))\n",
    "\n",
    "get_question_length('foo bar baz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf/Idf features.\n",
    "\n",
    "The usage is to call clear_tfidf_model(), followed by compute_tf_idf_model() on the entire corpus. Then, calling get_tf_idf_sequence() on a sequence of tokens will return the tf/idf score for each token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "TF_IDF_MODEL = None\n",
    "def clear_tfidf_model():\n",
    "    global TF_IDF_MODEL\n",
    "    TF_IDF_MODEL = None\n",
    "\n",
    "def compute_tf_idf_model(text):\n",
    "    global TF_IDF_MODEL\n",
    "    TF_IDF_MODEL = TfidfVectorizer(max_df=0.95, #min_df=5,\n",
    "                                   max_features=200000,\n",
    "                                   stop_words='english', use_idf=True)\n",
    "    \n",
    "    TF_IDF_MODEL.fit_transform(text)\n",
    "    \n",
    "def get_tf_idf_seq(texts, pad_length):\n",
    "    global TF_IDF_MODEL\n",
    "    seqs = []\n",
    "    for text in texts:\n",
    "        if type(text) != type('s'):\n",
    "            text = ''\n",
    "        tf_idf_scores = TF_IDF_MODEL.transform([text])\n",
    "        seq = []\n",
    "        tokens = text.split(' ')\n",
    "        tokens = tokens[-1*pad_length:]\n",
    "        for i in range(0, pad_length - len(tokens), 1):\n",
    "            seq.append([0.0])\n",
    "        for i in range(0, len(tokens), 1):\n",
    "            word = tokens[i]\n",
    "            if word in TF_IDF_MODEL.vocabulary_:\n",
    "                seq.append([tf_idf_scores[0, TF_IDF_MODEL.vocabulary_[word]]])\n",
    "            else:\n",
    "                seq.append([0.0])\n",
    "        seqs.append(seq)\n",
    "    return np.array(seqs)\n",
    "\n",
    "clear_tfidf_model()\n",
    "compute_tf_idf_model(['This is a really nice wii you have', 'I want to buy a sandwich'])\n",
    "print(get_tf_idf_seq(['wii a sandwich', 'astrology : capricorn sun cap moon cap rising say'], 10).shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell attempts to remove words not found in the Word2Vec model from the word index. It has limited success in its current form, not really contributing much to the model's accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3}\n",
      "[[0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0]]\n",
      "4\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "def reassign_word_index(word_index, embedding_model):\n",
    "    new_word_index = {}\n",
    "    new_index = 1\n",
    "    for word in word_index:\n",
    "        if word in embedding_model.vocab:\n",
    "            new_word_index[word] = new_index\n",
    "            new_index += 1\n",
    "    return new_word_index\n",
    "\n",
    "def make_sequences(texts1, texts2, word_index, maxlen):\n",
    "    if len(texts1) != len(texts2):\n",
    "        raise ValueError\n",
    "    texts1_seq = []\n",
    "    texts2_seq = []\n",
    "    max_size = 0\n",
    "    for i in range(0, len(texts1), 1):\n",
    "        if type(texts1[i] != type('s')):\n",
    "            texts1[i] = ''\n",
    "        if type(texts2[i] != type('s')):\n",
    "            texts2[i] = ''\n",
    "        text1_tokens = texts1[i].split()\n",
    "        text2_tokens = texts2[i].split()\n",
    "        max_index = len(word_index) + 1\n",
    "        additional_mappings = {}\n",
    "        text1_seq = []\n",
    "        text2_seq = []\n",
    "        for i in range(0, maxlen-len(text1_tokens), 1):\n",
    "            text1_seq.append(0)\n",
    "            \n",
    "        text1_tokens = text1_tokens[-1*maxlen:]\n",
    "        for word in text1_tokens:\n",
    "            if word in word_index:\n",
    "                text1_seq.append(word_index[word])\n",
    "            else:\n",
    "                if word not in additional_mappings:\n",
    "                    additional_mappings[word] = max_index\n",
    "                    max_index += 1\n",
    "                text1_seq.append(additional_mappings[word])\n",
    "\n",
    "        for i in range(0, maxlen-len(text2_tokens), 1):\n",
    "            text2_seq.append(0)\n",
    "\n",
    "        text2_tokens = text2_tokens[-1*maxlen:]\n",
    "        for word in text2_tokens:\n",
    "            if word in word_index:\n",
    "                text2_seq.append(word_index[word])\n",
    "            else:\n",
    "                if word not in additional_mappings:\n",
    "                    additional_mappings[word] = max_index\n",
    "                    max_index += 1\n",
    "                text2_seq.append(additional_mappings[word])\n",
    "\n",
    "        texts1_seq.append(text1_seq)\n",
    "        texts2_seq.append(text2_seq)\n",
    "        if max_index > max_size:\n",
    "            max_size = max_index\n",
    "        \n",
    "    return np.array(texts1_seq), np.array(texts2_seq), max_size \n",
    "\n",
    "\n",
    "class dummy_embedding_model:\n",
    "    def __init__(self):\n",
    "        self.vocab = {'a', 'b', 'c'}\n",
    "\n",
    "de = dummy_embedding_model()\n",
    "wi = {'a': 1, 'd': 2, 'b': 3, 'c': 4, 'e': 5}\n",
    "r = reassign_word_index(wi, de)\n",
    "s1, s2, sz = make_sequences(['a b d e'], ['a c d f g'], r, 6)\n",
    "print(r)\n",
    "print(s1)\n",
    "print(s2) \n",
    "print(sz)\n",
    "print(type(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The start of the pipeline. Reads in the data.\n",
    "\n",
    "The following cells compute all the features. They take some time to run. Go make a sandwich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and models...\n",
      "(417040, 6)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_FILE = '../data/augmented.csv'\n",
    "TEST_DATA_FILE = '../data/test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "TRAIN_PERCENTAGE = 0.90\n",
    "\n",
    "print('Loading data and models...')\n",
    "train_data = pd.read_csv('../data/augmented.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determing question types...\n",
      "(417040, 6)\n",
      "(417040, 6)\n",
      "(417040, 12)\n",
      "(417040, 12)\n",
      "Processed q1 of train.\n",
      "(417040, 18)\n",
      "Processed q2 of train.\n",
      "Processed q1 of test.\n",
      "Processed q2 of test.\n"
     ]
    }
   ],
   "source": [
    "print('Determing question types...')\n",
    "dummy = train_data.apply(lambda r: get_question_type(r['question1'], '1'), axis=1)\n",
    "print(dummy.shape)\n",
    "print(train_data.shape)\n",
    "train_data = pd.concat([train_data, dummy], axis=1)\n",
    "print(train_data.shape)\n",
    "print(train_data.shape)\n",
    "print('Processed q1 of train.')\n",
    "dummy2 = train_data.apply(lambda r: get_question_type(r['question2'], '2'), axis=1)\n",
    "train_data = pd.concat([train_data, dummy2], axis=1)\n",
    "print(train_data.shape)\n",
    "print('Processed q2 of train.')\n",
    "dummy3 = test_data.apply(lambda r: get_question_type(r['question1'], '1'), axis=1)\n",
    "test_data = pd.concat([test_data, dummy3], axis=1)\n",
    "print('Processed q1 of test.')\n",
    "dummy4 = test_data.apply(lambda r: get_question_type(r['question2'], '2'), axis=1)\n",
    "test_data = pd.concat([test_data, dummy4], axis=1)\n",
    "print('Processed q2 of test.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning q1 of train\n",
      "(417040, 19)\n",
      "Lemmatizing q1 of train\n",
      "(417040, 20)\n",
      "Cleaning q2 of train\n",
      "(417040, 21)\n",
      "Lemmatizing q2 of train\n",
      "(417040, 22)\n",
      "Cleaning q1 of test\n",
      "Lemmatizing q1 of test\n",
      "Cleaning q2 of test\n",
      "Lemmatizing q2 of test\n"
     ]
    }
   ],
   "source": [
    "print('Cleaning q1 of train')\n",
    "train_data['model_cleaned_question_1'] = train_data.apply(lambda r: clean_text(r['question1']), axis=1)\n",
    "print(train_data.shape)\n",
    "print('Lemmatizing q1 of train')\n",
    "train_data['cleaned_question_1'] = train_data.apply(lambda r: do_lemmatization(r['model_cleaned_question_1']), axis=1)\n",
    "print(train_data.shape)\n",
    "\n",
    "print('Cleaning q2 of train')\n",
    "train_data['model_cleaned_question_2'] = train_data.apply(lambda r: clean_text(r['question2']), axis=1)\n",
    "print(train_data.shape)\n",
    "print('Lemmatizing q2 of train')\n",
    "train_data['cleaned_question_2'] = train_data.apply(lambda r: do_lemmatization(r['model_cleaned_question_2']), axis=1)\n",
    "print(train_data.shape)\n",
    "\n",
    "print('Cleaning q1 of test')\n",
    "test_data['model_cleaned_question_1'] = test_data.apply(lambda r: clean_text(r['question1']), axis=1)\n",
    "print('Lemmatizing q1 of test')\n",
    "test_data['cleaned_question_1'] = test_data.apply(lambda r: do_lemmatization(r['model_cleaned_question_1']), axis=1)\n",
    "\n",
    "print('Cleaning q2 of test')\n",
    "test_data['model_cleaned_question_2'] = test_data.apply(lambda r: clean_text(r['question2']), axis=1)\n",
    "print('Lemmatizing q2 of test')\n",
    "test_data['cleaned_question_2'] = test_data.apply(lambda r: do_lemmatization(r['model_cleaned_question_2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Longest Common Substring scores...\n",
      "(417040, 23)\n"
     ]
    }
   ],
   "source": [
    "print('Computing Longest Common Substring scores...')\n",
    "train_data['longest_common_substring'] = train_data.apply(lambda r: longest_common_substring(r['cleaned_question_1'], r['cleaned_question_2']), axis=1)\n",
    "print(train_data.shape)\n",
    "test_data['longest_common_substring'] = test_data.apply(lambda r: longest_common_substring(r['cleaned_question_1'], r['cleaned_question_2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing IoU scores...\n",
      "(417040, 24)\n"
     ]
    }
   ],
   "source": [
    "print('Computing IoU scores...')\n",
    "train_data['IoU'] = train_data.apply(lambda r: intersection_over_union(r['cleaned_question_1'], r['cleaned_question_2']), axis=1)\n",
    "print(train_data.shape)\n",
    "test_data['IoU'] = test_data.apply(lambda r: intersection_over_union(r['cleaned_question_1'], r['cleaned_question_2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing question lengths...\n",
      "(417040, 25)\n",
      "(417040, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(417040, 26)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Computing question lengths...')\n",
    "train_data['question_1_length'] = train_data.apply(lambda r: get_question_length(r['cleaned_question_1']), axis=1)\n",
    "print(train_data.shape)\n",
    "train_data['question_2_length'] = train_data.apply(lambda r: get_question_length(r['cleaned_question_2']), axis=1)\n",
    "print(train_data.shape)\n",
    "test_data['question_1_length'] = test_data.apply(lambda r: get_question_length(r['cleaned_question_1']), axis=1)\n",
    "test_data['question_2_length'] = test_data.apply(lambda r: get_question_length(r['cleaned_question_2']), axis=1)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>is_how_question_1</th>\n",
       "      <th>is_what_question_1</th>\n",
       "      <th>is_when_question_1</th>\n",
       "      <th>is_where_question_1</th>\n",
       "      <th>...</th>\n",
       "      <th>is_who_question_2</th>\n",
       "      <th>is_why_question_2</th>\n",
       "      <th>model_cleaned_question_1</th>\n",
       "      <th>cleaned_question_1</th>\n",
       "      <th>model_cleaned_question_2</th>\n",
       "      <th>cleaned_question_2</th>\n",
       "      <th>longest_common_substring</th>\n",
       "      <th>IoU</th>\n",
       "      <th>question_1_length</th>\n",
       "      <th>question_2_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>astrology : capricorn sun cap moon cap rising say</td>\n",
       "      <td>astrology : capricorn sun cap moon cap rise say</td>\n",
       "      <td>triple capricorn sun moon ascendant capricorn say</td>\n",
       "      <td>triple capricorn sun moon ascendant capricorn say</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>good geologist</td>\n",
       "      <td>good geologist</td>\n",
       "      <td>great geologist</td>\n",
       "      <td>great geologist</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>How can I see all my Youtube comments?</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>read find youtube comments</td>\n",
       "      <td>read find youtube comment</td>\n",
       "      <td>see youtube comments</td>\n",
       "      <td>see youtube comment</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>make physics easy learn</td>\n",
       "      <td>make physic easy learn</td>\n",
       "      <td>make physics easy learn</td>\n",
       "      <td>make physic easy learn</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>114035</td>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>Is there a way to make learning physics easier?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>make physics easy learn</td>\n",
       "      <td>make physic easy learn</td>\n",
       "      <td>way make learning physics easier</td>\n",
       "      <td>way make learn physic easier</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>114035</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "      <td>Is there a way to make learning physics easier?</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>make physics easy learn</td>\n",
       "      <td>make physic easy learn</td>\n",
       "      <td>way make learning physics easier</td>\n",
       "      <td>way make learn physic easier</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>first sexual experience like</td>\n",
       "      <td>first sexual experience like</td>\n",
       "      <td>first sexual experience</td>\n",
       "      <td>first sexual experience</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>50277</td>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What is your first sexual experience?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>first sexual experience like</td>\n",
       "      <td>first sexual experience like</td>\n",
       "      <td>first sexual experience</td>\n",
       "      <td>first sexual experience</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>50277</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "      <td>What is your first sexual experience?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>first sexual experience</td>\n",
       "      <td>first sexual experience</td>\n",
       "      <td>first sexual experience</td>\n",
       "      <td>first sexual experience</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>6937</td>\n",
       "      <td>38502</td>\n",
       "      <td>How will Trump’s presidency affect international students in the US?</td>\n",
       "      <td>What does Trump’s victory mean for international students?</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>trump presidency affect international students us</td>\n",
       "      <td>trump presidency affect international student us</td>\n",
       "      <td>trump victory mean international students</td>\n",
       "      <td>trump victory mean international student</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>6937</td>\n",
       "      <td>88834</td>\n",
       "      <td>How will Trump’s presidency affect international students in the US?</td>\n",
       "      <td>How would a Trump presidency affect schools accessibility for international students?</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>trump presidency affect international students us</td>\n",
       "      <td>trump presidency affect international student us</td>\n",
       "      <td>would trump presidency affect schools accessibility international students</td>\n",
       "      <td>would trump presidency affect school accessibility international student</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  qid1    qid2  \\\n",
       "0   0   11    12       \n",
       "1   1   15    16       \n",
       "2   2   23    24       \n",
       "3   3   25    26       \n",
       "4   4   25    114035   \n",
       "5   5   26    114035   \n",
       "6   6   27    28       \n",
       "7   7   27    50277    \n",
       "8   8   28    50277    \n",
       "9   9   6937  38502    \n",
       "10  10  6937  88834    \n",
       "\n",
       "                                                                                 question1  \\\n",
       "0   Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?   \n",
       "1   How can I be a good geologist?                                                           \n",
       "2   How do I read and find my YouTube comments?                                              \n",
       "3   What can make Physics easy to learn?                                                     \n",
       "4   What can make Physics easy to learn?                                                     \n",
       "5   How can you make physics easy to learn?                                                  \n",
       "6   What was your first sexual experience like?                                              \n",
       "7   What was your first sexual experience like?                                              \n",
       "8   What was your first sexual experience?                                                   \n",
       "9   How will Trump’s presidency affect international students in the US?                     \n",
       "10  How will Trump’s presidency affect international students in the US?                     \n",
       "\n",
       "                                                                                     question2  \\\n",
       "0   I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?   \n",
       "1   What should I do to be a great geologist?                                                    \n",
       "2   How can I see all my Youtube comments?                                                       \n",
       "3   How can you make physics easy to learn?                                                      \n",
       "4   Is there a way to make learning physics easier?                                              \n",
       "5   Is there a way to make learning physics easier?                                              \n",
       "6   What was your first sexual experience?                                                       \n",
       "7   What is your first sexual experience?                                                        \n",
       "8   What is your first sexual experience?                                                        \n",
       "9   What does Trump’s victory mean for international students?                                   \n",
       "10  How would a Trump presidency affect schools accessibility for international students?        \n",
       "\n",
       "    is_duplicate  is_how_question_1  is_what_question_1  is_when_question_1  \\\n",
       "0   1             0.0                1.0                 0.0                  \n",
       "1   1             1.0                0.0                 0.0                  \n",
       "2   1             1.0                0.0                 0.0                  \n",
       "3   1             0.0                1.0                 0.0                  \n",
       "4   1             0.0                1.0                 0.0                  \n",
       "5   1             1.0                0.0                 0.0                  \n",
       "6   1             0.0                1.0                 0.0                  \n",
       "7   1             0.0                1.0                 0.0                  \n",
       "8   1             0.0                1.0                 0.0                  \n",
       "9   1             1.0                0.0                 0.0                  \n",
       "10  1             1.0                0.0                 0.0                  \n",
       "\n",
       "    is_where_question_1        ...         is_who_question_2  \\\n",
       "0   0.0                        ...         0.0                 \n",
       "1   0.0                        ...         0.0                 \n",
       "2   0.0                        ...         0.0                 \n",
       "3   0.0                        ...         0.0                 \n",
       "4   0.0                        ...         0.0                 \n",
       "5   0.0                        ...         0.0                 \n",
       "6   0.0                        ...         0.0                 \n",
       "7   0.0                        ...         0.0                 \n",
       "8   0.0                        ...         0.0                 \n",
       "9   0.0                        ...         0.0                 \n",
       "10  0.0                        ...         0.0                 \n",
       "\n",
       "    is_why_question_2                            model_cleaned_question_1  \\\n",
       "0   0.0                astrology : capricorn sun cap moon cap rising say    \n",
       "1   0.0                good geologist                                       \n",
       "2   0.0                read find youtube comments                           \n",
       "3   0.0                make physics easy learn                              \n",
       "4   0.0                make physics easy learn                              \n",
       "5   0.0                make physics easy learn                              \n",
       "6   0.0                first sexual experience like                         \n",
       "7   0.0                first sexual experience like                         \n",
       "8   0.0                first sexual experience                              \n",
       "9   0.0                trump presidency affect international students us    \n",
       "10  0.0                trump presidency affect international students us    \n",
       "\n",
       "                                  cleaned_question_1  \\\n",
       "0   astrology : capricorn sun cap moon cap rise say    \n",
       "1   good geologist                                     \n",
       "2   read find youtube comment                          \n",
       "3   make physic easy learn                             \n",
       "4   make physic easy learn                             \n",
       "5   make physic easy learn                             \n",
       "6   first sexual experience like                       \n",
       "7   first sexual experience like                       \n",
       "8   first sexual experience                            \n",
       "9   trump presidency affect international student us   \n",
       "10  trump presidency affect international student us   \n",
       "\n",
       "                                                       model_cleaned_question_2  \\\n",
       "0   triple capricorn sun moon ascendant capricorn say                             \n",
       "1   great geologist                                                               \n",
       "2   see youtube comments                                                          \n",
       "3   make physics easy learn                                                       \n",
       "4   way make learning physics easier                                              \n",
       "5   way make learning physics easier                                              \n",
       "6   first sexual experience                                                       \n",
       "7   first sexual experience                                                       \n",
       "8   first sexual experience                                                       \n",
       "9   trump victory mean international students                                     \n",
       "10  would trump presidency affect schools accessibility international students    \n",
       "\n",
       "                                                          cleaned_question_2  \\\n",
       "0   triple capricorn sun moon ascendant capricorn say                          \n",
       "1   great geologist                                                            \n",
       "2   see youtube comment                                                        \n",
       "3   make physic easy learn                                                     \n",
       "4   way make learn physic easier                                               \n",
       "5   way make learn physic easier                                               \n",
       "6   first sexual experience                                                    \n",
       "7   first sexual experience                                                    \n",
       "8   first sexual experience                                                    \n",
       "9   trump victory mean international student                                   \n",
       "10  would trump presidency affect school accessibility international student   \n",
       "\n",
       "    longest_common_substring       IoU question_1_length question_2_length  \n",
       "0   0.222222                  0.400000  9                 7                 \n",
       "1   0.500000                  0.333333  2                 2                 \n",
       "2   0.500000                  0.400000  4                 3                 \n",
       "3   1.000000                  1.000000  4                 4                 \n",
       "4   0.200000                  0.500000  4                 5                 \n",
       "5   0.200000                  0.500000  4                 5                 \n",
       "6   0.750000                  0.750000  4                 3                 \n",
       "7   0.750000                  0.750000  4                 3                 \n",
       "8   1.000000                  1.000000  3                 3                 \n",
       "9   0.333333                  0.375000  6                 5                 \n",
       "10  0.375000                  0.555556  6                 8                 \n",
       "\n",
       "[11 rows x 26 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint the datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417040, 26)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "train_data.to_csv('../data/train_data_with_features.csv', index=False, quoting=csv.QUOTE_ALL)\n",
    "test_data.to_csv('../data/test_data_with_features.csv', index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the deep learning input files. Convert the token sequences to one-hot vectors. Also save the tf/idf sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec model...\n",
      "Tokenizing..\n",
      "Found 120376 unique tokens\n",
      "Shape of data tensor: (417040, 30)\n",
      "Shape of label tensor: (417040,)\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 61775\n",
      "Dividing up train/val sets\n",
      "Saving to binary files...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/train_data_with_features.csv', index=False, quoting=csv.QUOTE_ALL)\n",
    "test_data = pd.read_csv('../data/test_data_with_features.csv', index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "EMBEDDING_FILE = '../models/GoogleNews-vectors-negative300.bin'\n",
    "EMBEDDING_DIM = 300\n",
    "TRAIN_DATA_FILE = '../data/augmented.csv'\n",
    "TEST_DATA_FILE = '../data/test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "print('Loading word2vec model...')\n",
    "sys.stdout.flush()\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "print('Tokenizing..')\n",
    "sys.stdout.flush()\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(\\\n",
    "    list(train_data['model_cleaned_question_1'])+\\\n",
    "    list(train_data['model_cleaned_question_2'])+\\\n",
    "    list(test_data['model_cleaned_question_1'])+\\\n",
    "    list(test_data['model_cleaned_question_2']))\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(list(train_data['model_cleaned_question_1']))\n",
    "sequences_2 = tokenizer.texts_to_sequences(list(train_data['model_cleaned_question_2']))\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(list(test_data['model_cleaned_question_1']))\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(list(test_data['model_cleaned_question_2']))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "sys.stdout.flush()\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(train_data['is_duplicate'])\n",
    "ids = np.array(train_data['id'])\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "sys.stdout.flush()\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test_data['test_id'])\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "sys.stdout.flush()\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "not_found_words = set()\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    else:\n",
    "        not_found_words.add(word)\n",
    "print('Null word embeddings: {0}'.format(len(not_found_words)))\n",
    "sys.stdout.flush()    \n",
    "\n",
    "with open('null_words.txt', 'w') as f:\n",
    "    for word in not_found_words:\n",
    "        f.write('{0}\\n'.format(word))\n",
    "\n",
    "    \n",
    "question_1_feature_columns = [\\\n",
    "    'is_who_question_1',\\\n",
    "    'is_what_question_1',\\\n",
    "    'is_where_question_1',\\\n",
    "    'is_when_question_1',\\\n",
    "    'is_why_question_1',\\\n",
    "    'is_how_question_1',\\\n",
    "    'question_1_length'\\\n",
    "]\n",
    "\n",
    "question_2_feature_columns = [\\\n",
    "    'is_who_question_2',\\\n",
    "    'is_what_question_2',\\\n",
    "    'is_where_question_2',\\\n",
    "    'is_when_question_2',\\\n",
    "    'is_why_question_2',\\\n",
    "    'is_how_question_2',\\\n",
    "    'question_2_length'\n",
    "]\n",
    "\n",
    "mutual_feature_columns = [\\\n",
    "    'longest_common_substring',\\\n",
    "    'IoU'\\\n",
    "]\n",
    "    \n",
    "print('Dividing up train/val sets')\n",
    "sys.stdout.flush()\n",
    "np.random.seed(1234)\n",
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "question_1_train_features = np.vstack((np.array(train_data[question_1_feature_columns].ix[idx_train]), np.array(train_data[question_2_feature_columns].ix[idx_train])))\n",
    "question_2_train_features = np.vstack((np.array(train_data[question_2_feature_columns].ix[idx_train]),np.array(train_data[question_1_feature_columns].ix[idx_train])))\n",
    "mutual_train_features = np.vstack((np.array(train_data[mutual_feature_columns].ix[idx_train]), np.array(train_data[mutual_feature_columns].ix[idx_train])))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "ids_val_final = np.concatenate((ids[idx_val], ids[idx_val]))\n",
    "\n",
    "question_1_val_features = np.vstack((np.array(train_data[question_1_feature_columns].ix[idx_val]), np.array(train_data[question_2_feature_columns].ix[idx_val])))\n",
    "question_2_val_features = np.vstack((np.array(train_data[question_2_feature_columns].ix[idx_val]), np.array(train_data[question_1_feature_columns].ix[idx_val])))\n",
    "mutual_val_features = np.vstack((np.array(train_data[mutual_feature_columns].ix[idx_val]), np.array(train_data[mutual_feature_columns].ix[idx_val])))\n",
    "\n",
    "test_question_1_features = np.array(test_data[question_1_feature_columns])\n",
    "test_question_2_features = np.array(test_data[question_2_feature_columns])\n",
    "test_mutual_features = np.array(test_data[mutual_feature_columns])\n",
    "\n",
    "print('Saving to binary files...')\n",
    "sys.stdout.flush()\n",
    "np.save('../scratch/data_1_train.npy', data_1_train)\n",
    "np.save('../scratch/data_2_train.npy', data_2_train)\n",
    "np.save('../scratch/labels_train.npy', labels_train)\n",
    "np.save('../scratch/question_1_train_features.npy', question_1_train_features)\n",
    "np.save('../scratch/question_2_train_features.npy', question_2_train_features)\n",
    "np.save('../scratch/mutual_train_features.npy', mutual_train_features)\n",
    "\n",
    "np.save('../scratch/data_1_val.npy', data_1_val)\n",
    "np.save('../scratch/data_2_val.npy', data_2_val)\n",
    "np.save('../scratch/labels_val.npy', labels_val)\n",
    "np.save('../scratch/question_1_val_features.npy', question_1_val_features)\n",
    "np.save('../scratch/question_2_val_features.npy', question_2_val_features)\n",
    "np.save('../scratch/mutual_val_features.npy', mutual_val_features)\n",
    "\n",
    "np.save('../scratch/ids_val.npy', ids_val_final)\n",
    "\n",
    "\n",
    "np.save('../scratch/test_data_1.npy', test_data_1)\n",
    "np.save('../scratch/test_data_2.npy', test_data_2)\n",
    "np.save('../scratch/test_ids.npy', test_ids)\n",
    "np.save('../scratch/test_question_1_features.npy', test_question_1_features)\n",
    "np.save('../scratch/test_question_2_features.npy', test_question_2_features)\n",
    "np.save('../scratch/test_mutual_features.npy', test_mutual_features)\n",
    "\n",
    "np.save('../scratch/embedding_matrix.npy', embedding_matrix)\n",
    "\n",
    "with open('../scratch/semaphore.txt', 'w') as f:\n",
    "    f.write('a')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative implementation that also saves tf/idf features and does out-of-vocabulary word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading features file...\n",
      "Loading word2vec model...\n",
      "Tokenizing..\n",
      "Crunching index...\n",
      "Generating sequences...\n",
      "(417040, 30)\n",
      "Preparing embedding matrix\n",
      "(58602, 301)\n",
      "Training TF/IDF model\n",
      "Generating tf/idf sequences\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "(417040, 30, 1)\n",
      "Dividing up train/val sets\n",
      "Saving to binary files...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Reading features file...')\n",
    "train_data = pd.read_csv('../data/train_data_with_features.csv', index_col=False, quoting=csv.QUOTE_ALL)\n",
    "test_data = pd.read_csv('../data/test_data_with_features.csv', index_col=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "EMBEDDING_FILE = '../models/GoogleNews-vectors-negative300.bin'\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "print('Loading word2vec model...')\n",
    "sys.stdout.flush()\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "print('Tokenizing..')\n",
    "sys.stdout.flush()\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "all_text = list(train_data['model_cleaned_question_1'])+\\\n",
    "    list(train_data['model_cleaned_question_2'])+\\\n",
    "    list(test_data['model_cleaned_question_1'])+\\\n",
    "    list(test_data['model_cleaned_question_2'])\n",
    "all_text = list(map(lambda r: '' if type(r) != type('s') or len(r) < 1 else r, all_text))\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "\n",
    "print('Crunching index...')\n",
    "new_word_index = reassign_word_index(tokenizer.word_index, word2vec)\n",
    "\n",
    "print('Generating sequences...')\n",
    "train_sequences_1_f, train_sequences_2_f, sz1 = make_sequences(list(train_data['model_cleaned_question_1']), list(train_data['model_cleaned_question_2']), new_word_index, MAX_SEQUENCE_LENGTH)\n",
    "train_sequences_1_b, train_sequences_2_b, sz2 = make_sequences(list(train_data['model_cleaned_question_2']), list(train_data['model_cleaned_question_1']), new_word_index, MAX_SEQUENCE_LENGTH)\n",
    "test_sequences_1_f, test_sequences_2_f, sz3 = make_sequences(list(test_data['model_cleaned_question_1']), list(test_data['model_cleaned_question_2']), new_word_index, MAX_SEQUENCE_LENGTH)\n",
    "test_sequences_1_b, test_sequences_2_b, sz4 = make_sequences(list(test_data['model_cleaned_question_1']), list(test_data['model_cleaned_question_2']), new_word_index, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(train_sequences_1_f.shape)\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "sys.stdout.flush()\n",
    "\n",
    "nb_extra_words = max(max(max(sz4, sz3), sz2), sz1) - len(new_word_index)\n",
    "nb_std_words = len(new_word_index)\n",
    "\n",
    "not_found_words = set()\n",
    "embedding_matrix = np.zeros((nb_std_words + nb_extra_words, EMBEDDING_DIM + nb_extra_words))\n",
    "\n",
    "for word, i in new_word_index.items():\n",
    "    embedding_matrix[i, 0:300] = word2vec.word_vec(word)\n",
    "\n",
    "for i in range(0, nb_extra_words, 1):\n",
    "    embedding_matrix[i+nb_std_words, i+EMBEDDING_DIM] = 1.0\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "    \n",
    "print('Training TF/IDF model')\n",
    "clear_tfidf_model()\n",
    "compute_tf_idf_model(all_text)\n",
    "\n",
    "print('Generating tf/idf sequences')\n",
    "tf_idf_train_sequences_1 = get_tf_idf_seq(list(train_data['model_cleaned_question_1']), MAX_SEQUENCE_LENGTH)\n",
    "print('...')\n",
    "tf_idf_train_sequences_2 = get_tf_idf_seq(list(train_data['model_cleaned_question_2']), MAX_SEQUENCE_LENGTH)\n",
    "print('...')\n",
    "tf_idf_test_sequences_1 = get_tf_idf_seq(list(test_data['model_cleaned_question_1']), MAX_SEQUENCE_LENGTH)\n",
    "print('...')\n",
    "tf_idf_test_sequences_2 = get_tf_idf_seq(list(test_data['model_cleaned_question_2']), MAX_SEQUENCE_LENGTH)\n",
    "print('...')\n",
    "\n",
    "print(tf_idf_train_sequences_1.shape)\n",
    "\n",
    "labels = np.array(train_data['is_duplicate'])\n",
    "ids = np.array(train_data['id'])\n",
    "test_ids = np.array(test_data['test_id'])\n",
    "    \n",
    "print('Dividing up train/val sets')\n",
    "sys.stdout.flush()\n",
    "np.random.seed(1234)\n",
    "perm = np.random.permutation(train_data.shape[0])\n",
    "idx_train = perm[:int(train_data.shape[0]*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(train_data.shape[0]*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((train_sequences_1_f[idx_train], train_sequences_1_b[idx_train]))\n",
    "data_2_train = np.vstack((train_sequences_2_f[idx_train], train_sequences_2_b[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "tfidf_1_train = np.vstack((tf_idf_train_sequences_1[idx_train], tf_idf_train_sequences_2[idx_train]))\n",
    "tfidf_2_train = np.vstack((tf_idf_train_sequences_2[idx_train], tf_idf_train_sequences_1[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((train_sequences_1_f[idx_val], train_sequences_1_b[idx_val]))\n",
    "data_2_val = np.vstack((train_sequences_2_f[idx_val], train_sequences_2_b[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "tfidf_1_val = np.vstack((tf_idf_train_sequences_1[idx_val], tf_idf_train_sequences_2[idx_val]))\n",
    "tfidf_2_val = np.vstack((tf_idf_train_sequences_2[idx_val], tf_idf_train_sequences_1[idx_val]))\n",
    "\n",
    "ids_val_final = np.concatenate((ids[idx_val], ids[idx_val]))\n",
    "\n",
    "\n",
    "print('Saving to binary files...')\n",
    "sys.stdout.flush()\n",
    "np.save('../scratch/data_1_train.npy', data_1_train)\n",
    "np.save('../scratch/data_2_train.npy', data_2_train)\n",
    "np.save('../scratch/labels_train.npy', labels_train)\n",
    "np.save('../scratch/data_1_tfidf_train.npy', tfidf_1_train)\n",
    "np.save('../scratch/data_2_tfidf_train.npy', tfidf_2_train)\n",
    "\n",
    "np.save('../scratch/data_1_val.npy', data_1_val)\n",
    "np.save('../scratch/data_2_val.npy', data_2_val)\n",
    "np.save('../scratch/labels_val.npy', labels_val)\n",
    "np.save('../scratch/data_1_val_tfidf.npy', tfidf_1_val)\n",
    "np.save('../scratch/data_2_val_tfidf.npy', tfidf_2_val)\n",
    "\n",
    "np.save('../scratch/ids_val.npy', ids_val_final)\n",
    "\n",
    "\n",
    "np.save('../scratch/test_data_1.npy', test_sequences_1_f)\n",
    "np.save('../scratch/test_data_2.npy', test_sequences_2_f)\n",
    "np.save('../scratch/test_ids.npy', test_ids)\n",
    "np.save('../scratch/test_data_tfidf_1.npy', tf_idf_test_sequences_1)\n",
    "np.save('../scratch/test_data_tfidf_2.npy', tf_idf_test_sequences_2)\n",
    "\n",
    "np.save('../scratch/embedding_matrix.npy', embedding_matrix)\n",
    "\n",
    "with open('../scratch/semaphore.txt', 'w') as f:\n",
    "    f.write('a')\n",
    "\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
